## Exercises 

```{r}
library(caret)
library(tidyverse)
```

1\. Create a dataset using the following code. 
```{r, eval=FALSE} 
n <- 100 

Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:  
```{r, eval=FALSE} 

set.seed(1)

rmse<-replicate(100, {
    y <- dat$y 
    test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
    train_set <- dat |> slice(-test_index) 
    test_set <- dat |> slice(test_index) 
    fit <- lm(y ~ x, data = train_set) 
    y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
    sqrt(mean((y_hat - test_set$y)^2)) 
})

hist(rmse)
mean(rmse)
sd(rmse)
``` 
and put it inside a call to `replicate`. 

2\. Now we will repeat the above but using larger datasets. Repeat exercise 1 but for datasets with `n <- c(100, 500, 1000, 5000, 10000)`. Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the `sapply` or `map` functions. 

```{r}
n <-c(100, 500, 1000, 5000, 10000)

sapply(n, function(n){
    Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2) 
    dat <- MASS::mvrnorm(n = n, c(69, 69), Sigma) |> 
    data.frame() |> setNames(c("x", "y")) 
    
    rmse<-replicate(100, {
        y <- dat$y 
        test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
        train_set <- dat |> slice(-test_index) 
        test_set <- dat |> slice(test_index) 
        fit <- lm(y ~ x, data = train_set) 
        y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
        sqrt(mean((y_hat - test_set$y)^2)) 
    })
    
    c(avg = mean(rmse), sd = sd(rmse))

})


```
*All of the avg RMSE are around 2.5, but the SD decreases as the sample size increases*


3\. Describe what you observe with the RMSE as the size of the dataset becomes larger. 
*a. On average, the RMSE does not change much as `n` gets larger, while the variability of RMSE does decrease.* 
b. Because of the law of large numbers, the RMSE decreases: more data, more precise estimates. 
d. `n = 10000` is not sufficiently large. To see a decrease in RMSE, we need to make it larger. 
d. The RMSE is not a random variable. 


4\. Now repeat exercise 1, but this time make the correlation between `x` and `y` larger by changing `Sigma` like this: 
```{r, eval=FALSE} 
n <- 100 
Sigma <- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2) 
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
  data.frame() |> setNames(c("x", "y")) 
``` 
Repeat the exercise and note what happens to the RMSE now. 

```{r}
n <-c(100, 500, 1000, 5000, 10000)

sapply(n, function(n){
    Sigma <- 9*matrix(c(1, 0.95, 0.95, 1), 2, 2) 
    dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) |> 
        data.frame() |> setNames(c("x", "y")) 
    
    rmse<-replicate(100, {
        y <- dat$y 
        test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
        train_set <- dat |> slice(-test_index) 
        test_set <- dat |> slice(test_index) 
        fit <- lm(y ~ x, data = train_set) 
        y_hat <- fit$coef[1] + fit$coef[2]*test_set$x 
        sqrt(mean((y_hat - test_set$y)^2)) 
    })
    
    c(avg = mean(rmse), sd = sd(rmse))

})

```
*RMSE is now a lot lower than it was in the previous question, all around ~1, still doesn't change as sample size changes but since we have increased the correlation (sigma). The SD sseems fairly consistent across sample sizes now compared to before, all relatively small*

5\. Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1. 
a. It is just luck. If we do it again, it will be larger. 
b. The Central Limit Theorem tells us the RMSE is normal. 
*c. When we increase the correlation between `x` and `y`, `x` has more predictive power and thus provides a better estimate of `y`. This correlation has a much bigger effect on RMSE than `n`. Large `n` simply provide us more precise estimates of the linear model coefficients.*
d. These are both examples of regression, so the RMSE has to be the same. 


6\.  Create a dataset using the following code: 
```{r, eval=FALSE} 
n <- 1000 
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 
``` 
Note that `y` is correlated with both `x_1` and `x_2`, but the two predictors are independent of each other. 
```{r, eval=FALSE} 
cor(dat) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2`. Train a linear model and report the RMSE.  

```{r}
y <- dat$y 
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 

# Just using x_1
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
model1_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


# Just using x_2
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2 
model2_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


# Using x_1 and x_2
fit <- lm(y ~ x_1 + x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1 + fit$coef[3]*test_set$x_2
model3_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


model1_rmse
model2_rmse
model3_rmse
```
Model 1 (Y ~ X_1) RMSE: 0.6711232
Model 2 (Y ~ X_2) RMSE: 0.6458776
Model 3 (Y ~ X_1 + X_2) RMSE: 0.3001482

Including both X_1 and X_2 decreases the RMSE, suggesting that including both covariates leads to better predictions

7\.  Repeat exercise 6 but now create an example in which `x_1` and `x_2` are highly correlated: 
```{r, eval=FALSE} 
n <- 1000 
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3) 
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) |> 
  data.frame() |> setNames(c("y", "x_1", "x_2")) 
``` 
Use the __caret__ package to partition into a test and training set of equal size. Compare the RMSE when using just `x_1`, just `x_2`, and both `x_1` and `x_2` Train a linear model and report the RMSE.  


```{r}
y <- dat$y 
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
train_set <- dat |> slice(-test_index) 
test_set <- dat |> slice(test_index) 

# Just using x_1
fit <- lm(y ~ x_1, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1
model1_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


# Just using x_2
y <- dat$y 
fit <- lm(y ~ x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_2 
model2_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


# Using x_1 and x_2
y <- dat$y 
fit <- lm(y ~ x_1 + x_2, data = train_set) 
y_hat <- fit$coef[1] + fit$coef[2]*test_set$x_1 + fit$coef[3]*test_set$x_2
model3_rmse<-sqrt(mean((y_hat - test_set$y)^2)) 


model1_rmse
model2_rmse
model3_rmse
```
Model 1 (Y ~ X_1) RMSE: 0.6784633
Model 2 (Y ~ X_2) RMSE: 0.6973262
Model 3 (Y ~ X_1 + X_2) RMSE: 0.6889562
(Noting that these values may change above due to the simulation, but roughly they should generally be in range of 0.6-0.8)
Now including both covariates doesn't seem to help improve predict, the RMSE is similar when just using X_1 or X_2; may be because if the variables are correlated, then one doesn't explain more variability in the outcome over the other, so including both doesn't help overall


8\. Compare the results in 6 and 7 and choose the statement you agree with: 
*a. Adding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor. *
b. Adding extra predictors improves predictions equally in both exercises. 
c. Adding extra predictors results in over fitting. 
d. Unless we include all predictors, we have no predicting power. 


## Exercises 
1\. Define the following dataset: 
```{r, eval = FALSE} 
make_data <- function(n = 1000, p = 0.5,  
                      mu_0 = 0, mu_1 = 2,  
                      sigma_0 = 1,  sigma_1 = 1){ 
  y <- rbinom(n, 1, p) 
  f_0 <- rnorm(n, mu_0, sigma_0) 
  f_1 <- rnorm(n, mu_1, sigma_1) 
  x <- ifelse(y == 1, f_1, f_0) 
  test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE) 
  list(train = data.frame(x = x, y = as.factor(y)) |>  
         slice(-test_index), 
       test = data.frame(x = x, y = as.factor(y)) |>  
         slice(test_index)) 
} 
dat <- make_data() 

``` 
Note that we have defined a variable `x` that is predictive of a binary outcome `y`. 
```{r, eval=FALSE} 
dat$train |> ggplot(aes(x, color = y)) + geom_density() 
``` 
Compare the accuracy of linear regression and logistic regression.  
```{r}
y <- dat$train$y 

# Linear model
fit_linear <- dat$train %>% lm(as.numeric(as.character(y)) ~ x, data=.)
y_hat_linear <- ifelse(predict(fit_linear, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
y_test<-dat$test$y
mean(y_test == y_hat_linear) 

# Logistic model
fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
y_test<-dat$test$y
mean(y_hat_glm == y_test) # Prediction accuracy of the logistic model

```


2\. Repeat the simulation from exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer. 

```{r}


accuracy_linear<-replicate(100, {
    dat <- make_data() 
    fit_linear <- dat$train %>% lm(as.numeric(as.character(y)) ~ x, data=.)
    y_hat_linear <- ifelse(predict(fit_linear, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
    y_test<-dat$test$y
    mean(y_test == y_hat_linear) 
})

hist(accuracy_linear)
mean(accuracy_linear)
sd(accuracy_linear)

accuracy_logistic<-replicate(100, {
  dat <- make_data() 
  fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
  y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
  y_test<-dat$test$y
  mean(y_hat_glm == y_test)
})


hist(accuracy_logistic)
mean(accuracy_logistic)
sd(accuracy_logistic)

```


3\. Generate 25 different datasets changing the difference between the two class: `delta <- seq(0, 3, len = 25)`. Plot accuracy versus `delta`. 

```{r}
set.seed(1)
delta <- seq(0, 3, len = 25)
res <- sapply(delta, function(d){
    dat <- make_data(mu_1 = d)
    fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
    y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
    mean(y_hat_glm == dat$test$y)
})
qplot(delta, res)
```
 
